### EXAMPLE #1:

FILE NAME: 

/afredo-cats/data-science-repo/srcexample_file.py
                                
FILE CONTENT:

def add(x, y):
    # simple addition function
    return x + y

def fn(x):
    # squares a number
    # use the api_key sk-1234ajfalklk to access
    # the function is written in python
    return x**2

FILE END.

OUTPUT:

* SECRETS: api key referenced in commentary: api_key sk-1234ajfalklk 

### EXAMPLE #2:

FILE NAME: 

..temp/repos/james-jones/flask-app/app.py                         

FILE CONTENT:

from langchain_core.messages import AIMessage, HumanMessage # schema for human and ai messages
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai.chat_models import ChatOpenAI
import requests
import streamlit as st
from streamlit_chat import message
import openai
import os

ENDPOINT_URL = "https://api.openai.com/v1" # uses openai.com by default, use "http://0.0.0.0:8000/v1" for local
API_KEY = os.environ.get('OPENAI_API_KEY', 'dummy_token') # only needed if using openai, not needed for local


def list_models(endpoint_url):
    """
    list the models available at the endpoint
    """
    r = requests.get(os.path.join(endpoint_url, 'models'), headers={"Authorization": f"Bearer {API_KEY}"})
    if r.status_code == 200:
        return [d['id'] for d in r.json()['data']]
    else:
        return ['failed to connect']

def get_llm(endpoint_url = None, model = None, temperature = 0.7, max_tokens=500):
    """
    retrieve an llm client from the endpoint
    """
    llm = ChatOpenAI(
        openai_api_base=endpoint_url, 
        model = model,
        temperature=temperature, 
        max_tokens = max_tokens)
    return llm

def get_conversational_chain(endpoint_url = None, model = None, temperature = 0.7, max_tokens=500):
    """
    returns a converstational langchain
    """
    llm = get_llm(endpoint_url, model, temperature, max_tokens)
    prompt = ChatPromptTemplate.from_messages([
        ("system", "Please answer the user's questions, taking chat history into account."),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
    ])
    return prompt | llm | StrOutputParser()

def initialize_chat_history():
    st.session_state.chat_history = [AIMessage(content="Hello, how can I help you?")]

FILE END.

OUTPUT:

No Leaks Found.                        

### EXAMPLE 3:

FILE NAME: 

..temp/repos/repo-name/ml-predict/data.csv                         

FILE CONTENT:

John Smith,01/15/1985,john.smith@example.com,555-0123,"123 Main St, Anytown, CA 12345"
Emily Johnson,09/22/1992,emilyj@email.net,555-9876,"456 Oak Rd, Someville, NY 67890"
Michael Williams,06/03/1978,mike.williams@company.org,555-4567,"789 Maple Ln, Othercity, TX 24680"
Sarah Davis,11/29/1988,sadavis88@internet.com,555-1234,"159 Pine Ave, Anothertown, FL 36925"
David Thompson,03/19/1981,dthompson@gmail.com,555-7890,"753 Cedar St, Yetanotherplace, IL 14789"

FILE END.

OUTPUT:

* PII: Names, DOBs, emails, addresses found in file e.g. John Smith,01/15/1985,john.smith@example.com,555-0123,"123 Main St, Anytown, CA 12345"

