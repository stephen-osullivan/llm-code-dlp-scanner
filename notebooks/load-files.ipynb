{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =\"\"\"\n",
    "Your sole task is to analyze the given code base and identify any instances of potential leaks of confidential or sensitive information, such as API keys, access tokens, passwords, encryption keys, or any other secrets that should not be exposed. You must not provide any additional information, summaries, recommendations, commentary, or descriptions about the code beyond reporting the identified leaks.\n",
    "\n",
    "Please provide your output in the following format:\n",
    "\n",
    "<line_number>: <leaked_information>\n",
    "\n",
    "Where:\n",
    "- <line_number> is the line number in the code where the leak was found\n",
    "- <leaked_information> is the actual sensitive information that was leaked (e.g., API key string, password, encryption key)\n",
    "\n",
    "If no leaks are found, simply respond with \"No leaks found.\"\n",
    "\n",
    "Do not provide any output other than the specified format. Strictly follow the output format and do not include any additional text or information whatsoever.\n",
    "\n",
    "Below is an example:\n",
    "\n",
    "FILE NAME: \n",
    "\n",
    "example_file.py\n",
    "                                \n",
    "FILE CONTENT:\n",
    "\n",
    "def add(x, y):\n",
    "    # simple addition function\n",
    "    return x + y\n",
    "\n",
    "def fn(x):\n",
    "    # squares a number\n",
    "    # use the api_key sk-1234ajfalklk to access\n",
    "    # the function is written in python\n",
    "    return x**2\n",
    "\n",
    "FILE END.\n",
    "\n",
    "* <7> : API key found : sk-1234ajfalklk                        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to decode: 02_output/nn_mnist_training.png\n",
      "Loaded 10 documents from the changed files.\n",
      "Lengths: [('../../temp/repos/stephen-osullivan/pytorch/.gitignore', 16), ('../../temp/repos/stephen-osullivan/pytorch/00_basics.ipynb', 7968), ('../../temp/repos/stephen-osullivan/pytorch/01_dataset_transformer.py', 1696), ('../../temp/repos/stephen-osullivan/pytorch/02_simple_nn_mnist.py', 3619), ('../../temp/repos/stephen-osullivan/pytorch/03_cnn_cifar10.ipynb', 5231), ('../../temp/repos/stephen-osullivan/pytorch/04_cnn_cifar10_transforms.ipynb', 5231), ('../../temp/repos/stephen-osullivan/pytorch/05_transfer_learning_cnn.ipynb', 8518), ('../../temp/repos/stephen-osullivan/pytorch/06_rnn_from_scratch.ipynb', 7362), ('../../temp/repos/stephen-osullivan/pytorch/requirements.txt', 58), ('../../temp/repos/stephen-osullivan/pytorch/torch_transfer_learning.ipynb', 7392)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='**/data/\\n*.model', metadata={'source': '../../temp/repos/stephen-osullivan/pytorch/.gitignore'}),\n",
       " Document(page_content='\\'markdown\\' cell: \\'[\\'# Pytorch Basics\\\\n\\', \\'\\\\n\\', \\'## Part 1: Basic Methods\\']\\'\\n\\n\\'code\\' cell: \\'[\\'##### basics ######\\\\n\\', \\'import torch\\\\n\\', \\'import torch.nn as nn\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# use gpu if available\\\\n\\', \"device = torch.device(\\'cpu\\')\\\\n\", \\'if torch.cuda.is_available():\\\\n\\', \\'    device = torch.device(\"cuda\")\\\\n\\', \\'print(device)\\']\\'\\n with output: \\'[\\'cpu\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# tensort attributes\\\\n\\', \"print(f\\'shape: {x.shape}, dtype: {x.dtype}, requires_grad: {x.requires_grad}, device: {x.device}\\')\"]\\'\\n with output: \\'[\\'shape: torch.Size([4, 5]), dtype: torch.float32, requires_grad: False, device: cpu\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# methods\\\\n\\', \\'x = torch.rand(4,5)\\\\n\\', \\'x_numpy = x.numpy() # return a copy of x as numpy\\\\n\\', \\'x = x.to(device) # return a copy of x on the device\\\\n\\', \"x = x.to(\\'cpu\\') # return a copy of x on the cpu\\\\n\", \\'x = x.view(5,4) # return a reshaped view of x\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# some math methods\\\\n\\', \\'x = torch.rand(4,5) # define x\\\\n\\', \\'x.T # transpose\\\\n\\', \\'x @ x.T # matrix multiplication\\\\n\\', \\'x.add(x) # add\\\\n\\', \\'x.add_(x) # method ending with \"_\" is in place\\\\n\\', \\'\\\\n\\', \\'max_vals, max_args = x.max(dim=1) # return max vals and indices by row\\\\n\\', \"print(\\'max_vals:\\', max_vals, \\'indices:\\', max_args)\"]\\'\\n with output: \\'[\\'max_vals: tensor([1.6570, 1.6633, 1.9301, 1.9928]) indices: tensor([0, 0, 1, 3])\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# getting values\\\\n\\', \"print(\\'x[0,0] :\\', x[0,0]) # this is a one element tensor\\\\n\", \"print(\\'x[0,0].item() :\\', x[0,0].item()) # this a numpy float\\\\n\"]\\'\\n with output: \\'[\\'x[0,0] : tensor(1.6570)\\\\n\\', \\'x[0,0].item() : 1.6570290327072144\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# gradients:\\\\n\\', \\'# lets calc dz/dx where z is scalar z(y(x))\\\\n\\', \\'# here z = sum(y**2) where y = x + 2\\\\n\\', \\'x = torch.rand(3, requires_grad=True) # gradient tracked: we want dz/dx later\\\\n\\', \"print(\\'x:\\', x, \\'x.grad:\\', x.grad) # shows required grad = true\\\\n\", \\'y = x + 2  \\\\n\\', \"print(\\'y:\\', y, \\'x.grad:\\', x.grad) # y now has grad_fn attribute <AddBackward>\\\\n\", \\'z = sum(y**2)\\\\n\\', \"print(\\'z:\\', z, \\'x.grad:\\', x.grad)  \\\\n\", \"print(\\'calling z.backward()\\')\\\\n\", \\'z.backward() # dz/dx is calculated and x.grad is now populated\\\\n\\', \"print(\\'(dz/dx) x_grad:\\', x.grad)\\\\n\", \"print(\\'check gradient:\\', x.grad == 2*(x+2)) # xgrad is dz/dx i.e. 2*(x+2)\"]\\'\\n with output: \\'[\\'x: tensor([0.6613, 0.1218, 0.5651], requires_grad=True) x.grad: None\\\\n\\', \\'y: tensor([2.6613, 2.1218, 2.5651], grad_fn=<AddBackward0>) x.grad: None\\\\n\\', \\'z: tensor(18.1647, grad_fn=<AddBackward0>) x.grad: None\\\\n\\', \\'calling z.backward()\\\\n\\', \\'(dz/dx) x_grad: tensor([5.3226, 4.2437, 5.1302])\\\\n\\', \\'check gradient: tensor([True, True, True])\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Part 2: Full Examples\\']\\'\\n\\n\\'code\\' cell: \\'[\\'##### linear regression using autograd #####\\\\n\\', \\'# true parameters\\\\n\\', \\'w_true = torch.tensor([1,2,3,4,5], dtype = torch.float32).view(-1,1)\\\\n\\', \\'b_true = torch.tensor(10, dtype = torch.float32)\\\\n\\', \\'\\\\n\\', \\'# X is input, y is target\\\\n\\', \\'X = torch.normal(mean=0, std=1, size=(100,5)) # n = 100, dim = 5\\\\n\\', \\'y = X @ w_true + b_true\\\\n\\', \\'\\\\n\\', \\'# define the model parameters\\\\n\\', \\'w_est = torch.normal(mean=0, std=0.1, size = (5,1), dtype= torch.float32, requires_grad = True)\\\\n\\', \\'b_est = torch.normal(mean=0, std=0.1, size = (1,1), dtype= torch.float32, requires_grad = True)\\\\n\\', \\'\\\\n\\', \\'# define the model\\\\n\\', \\'def forward(X):\\\\n\\', \\'    return X @ w_est + b_est\\\\n\\', \\'\\\\n\\', \\'# define the loss function\\\\n\\', \\'def loss(y, y_pred):\\\\n\\', \\'    return ((y-y_pred)**2).mean() # MSE\\\\n\\', \\'\\\\n\\', \\'# define the learning parameters\\\\n\\', \\'learning_rate, epochs = 0.03, 150\\\\n\\', \\'\\\\n\\', \\'# training loop\\\\n\\', \\'for epoch in range(epochs):\\\\n\\', \\'    #forward pass\\\\n\\', \\'    y_pred = forward(X)\\\\n\\', \\'    l = loss(y, y_pred)\\\\n\\', \\'\\\\n\\', \\'    #backward pass\\\\n\\', \\'    l.backward() # this calculate dl/dw and dl/db\\\\n\\', \\'\\\\n\\', \\'    #updates\\\\n\\', \\'    with torch.no_grad(): # this calc should not recalculate the gradients\\\\n\\', \\'        w_est -= learning_rate * w_est.grad\\\\n\\', \\'        b_est -= learning_rate * b_est.grad\\\\n\\', \\'    \\\\n\\', \\'    #zeroise grads (otherwise the gradient will be incremented each loop)\\\\n\\', \\'    w_est.grad.zero_(), b_est.grad.zero_() \\\\n\\', \\'\\\\n\\', \\'    #print output\\\\n\\', \\'    if((epoch + 1) %25 == 0): # console update every 20 iterations\\\\n\\', \"        param_string = \\' \\'.join([f\\'w_{i}: {w.item():.2f}\\' for i, w in enumerate(w_est)]) + f\\' b: {b_est.item():.2f}\\'\\\\n\", \"        print(f\\'epoch: {epoch + 1}, loss: {l:.2f}\\', \\'estimated params:\\', param_string)\\\\n\"]\\'\\n with output: \\'[\\'epoch: 25, loss: 6.58 estimated params: w_0: 1.38 w_1: 1.64 w_2: 2.76 w_3: 3.38 w_4: 3.92 b: 8.04\\\\n\\', \\'epoch: 50, loss: 0.30 estimated params: w_0: 1.17 w_1: 1.92 w_2: 3.03 w_3: 3.90 w_4: 4.72 b: 9.60\\\\n\\', \\'epoch: 75, loss: 0.02 estimated params: w_0: 1.05 w_1: 1.97 w_2: 3.02 w_3: 3.99 w_4: 4.92 b: 9.92\\\\n\\', \\'epoch: 100, loss: 0.00 estimated params: w_0: 1.02 w_1: 1.99 w_2: 3.01 w_3: 4.00 w_4: 4.98 b: 9.98\\\\n\\', \\'epoch: 125, loss: 0.00 estimated params: w_0: 1.00 w_1: 2.00 w_2: 3.00 w_3: 4.00 w_4: 4.99 b: 10.00\\\\n\\', \\'epoch: 150, loss: 0.00 estimated params: w_0: 1.00 w_1: 2.00 w_2: 3.00 w_3: 4.00 w_4: 5.00 b: 10.00\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'##### logistic regression using pytorch class structure #####\\\\n\\', \\'from sklearn.datasets import load_breast_cancer\\\\n\\', \\'from sklearn.model_selection import train_test_split\\\\n\\', \\'from sklearn.preprocessing import StandardScaler\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'# 0) prepare data\\\\n\\', \\'dataset = load_breast_cancer()\\\\n\\', \\'X, y = dataset.data, dataset.target\\\\n\\', \\'n_samples, n_features = X.shape \\\\n\\', \\'\\\\n\\', \\'X_train, X_test, y_train, y_test = train_test_split(\\\\n\\', \\'    X, y, test_size = 0.2, stratify = y, random_state = 123)\\\\n\\', \\'\\\\n\\', \\'# standardise data and convert to tensors\\\\n\\', \\'sc = StandardScaler()\\\\n\\', \\'X_train = sc.fit_transform(X_train)\\\\n\\', \\'X_test = sc.transform(X_test)\\\\n\\', \\'\\\\n\\', \\'X_train = torch.tensor(X_train, dtype=torch.float32)\\\\n\\', \\'X_test = torch.tensor(X_test, dtype=torch.float32)\\\\n\\', \\'y_train = torch.tensor(y_train, dtype=torch.float32).view(-1,1)\\\\n\\', \\'y_test = torch.tensor(y_test, dtype=torch.float32).view(-1,1)\\\\n\\', \\'\\\\n\\', \\'def calc_accuracy(y_prob, y_true):\\\\n\\', \\'    y_pred = y_prob.round() # round to 0 or 1\\\\n\\', \\'    acc = (y_pred == y_true).sum()/len(y_true)\\\\n\\', \\'    return acc\\\\n\\', \\'\\\\n\\', \\'# 1) model\\\\n\\', \\'class LogisticRegression(nn.Module):\\\\n\\', \\'    def __init__(self, n_input_features):\\\\n\\', \\'        # initialze superclass\\\\n\\', \\'        super(LogisticRegression, self).__init__()\\\\n\\', \\'        # initialize layer objects\\\\n\\', \\'        self.linear = nn.Linear(n_input_features, 1)\\\\n\\', \\'    \\\\n\\', \\'    def forward(self, X):\\\\n\\', \\'        out = torch.sigmoid(self.linear(X))\\\\n\\', \\'        return out\\\\n\\', \\'\\\\n\\', \\'model = LogisticRegression(n_features).to(device)\\\\n\\', \\'\\\\n\\', \\'# 2) loss and optimizer\\\\n\\', \\'criterion = nn.BCELoss() # binary cross entropy loss\\\\n\\', \\'optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\\\\n\\', \\'\\\\n\\', \\'# 3) training loop\\\\n\\', \\'num_epochs = 100\\\\n\\', \\'learning_rate = 0.01\\\\n\\', \\'for epoch in range(num_epochs):\\\\n\\', \\'    #forward pass\\\\n\\', \\'    X_train, y_train = X_train.to(device), y_train.to(device)\\\\n\\', \\'    y_prob = model(X_train)\\\\n\\', \\'    loss = criterion(y_prob, y_train)\\\\n\\', \\'    \\\\n\\', \\'    #backward pass\\\\n\\', \\'    loss.backward()\\\\n\\', \\'    \\\\n\\', \\'    #updates\\\\n\\', \\'    optimizer.step()\\\\n\\', \\'    optimizer.zero_grad()\\\\n\\', \\'    \\\\n\\', \\'    if((epoch +1) % 20 == 0):\\\\n\\', \"        with torch.no_grad(): # we don\\'t want to add to the gradients, having just zeroised them\\\\n\", \\'            X_test.to(device), y_test.to(device)\\\\n\\', \\'            train_acc = calc_accuracy(y_prob, y_train)\\\\n\\', \\'            test_acc = calc_accuracy(model(X_test), y_test)\\\\n\\', \"            print(f\\'epoch: {epoch +1}, loss: {loss.item():.2f}, train_acc:{train_acc:.2f}, test_acc:{test_acc:.2f}\\')\"]\\'\\n with output: \\'[\\'epoch: 20, loss: 0.11, train_acc:0.97, test_acc:0.96\\\\n\\', \\'epoch: 40, loss: 0.09, train_acc:0.98, test_acc:0.97\\\\n\\', \\'epoch: 60, loss: 0.08, train_acc:0.98, test_acc:0.97\\\\n\\', \\'epoch: 80, loss: 0.07, train_acc:0.98, test_acc:0.97\\\\n\\', \\'epoch: 100, loss: 0.07, train_acc:0.98, test_acc:0.97\\\\n\\']\\'\\n\\n', metadata={'source': '../../temp/repos/stephen-osullivan/pytorch/00_basics.ipynb'}),\n",
       " Document(page_content=\"# Here we defeine a custom dataset and cusomt transformer and demonstrate loading them into a dataloader\\n\\nimport torch, math, torchvision\\nimport torch.nn as nn\\nimport numpy as np\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom sklearn import datasets #used to get wine data\\n\\n# Datasets need at minimum an __init__, __getitem__ and __length__ method\\nclass WineDataset(Dataset):\\n    def __init__(self, transform=None):\\n        #data loading\\n        wine = datasets.load_wine()\\n        X, y = wine.data, wine.target\\n        self.X = X.astype(np.float32)\\n        self.y = y.astype(np.float32).reshape(-1,1)\\n        self.n_samples = X.shape[0]\\n        self.transform = transform\\n    \\n    def __getitem__(self, index):\\n        # allows calling of dataset[i]\\n        sample = self.X[index], self.y[index]\\n        if self.transform:\\n            sample = self.transform(sample)\\n        \\n        return sample\\n        \\n    def __len__(self):\\n        # allows calling len(dataset)\\n        return self.n_samples\\n\\n#define a tensor transform, transforms must have a __call__ method.\\nclass ToTensor:\\n    def __call__(self, sample):\\n        inputs, targets = sample\\n        return torch.from_numpy(inputs), torch.from_numpy(targets)\\n\\n# create the data loader: 4 obs in each batch, obj length = 45\\ndataloader = DataLoader(dataset = dataset, batch_size = 4, shuffle = True)\\n\\n#we can use the dataloader as an iterater\\nnum_epochs = 2\\ntotal_samples = len(dataset)\\nnum_iterations = math.ceil(total_samples/4)\\nfor epoch in range(num_epochs):\\n    for i, (inputs, labels) in enumerate(dataloader):\\n        if(i % 5 == 0):\\n            print(f'epoch: {epoch + 1}, batch: {i}, batchsize: {len(labels)}')\\n\\n            \\n\", metadata={'source': '../../temp/repos/stephen-osullivan/pytorch/01_dataset_transformer.py'}),\n",
       " Document(page_content='# simple nn on mnist, gets 97% test accuracy\\nimport torch\\nimport torch.nn as nn\\nimport torchvision # datasets\\nimport torchvision.transforms as transforms\\nimport matplotlib.pyplot as plt\\n\\n# use gpu if available\\ndevice = torch.device(\\'cpu\\')\\nif torch.cuda.is_available():\\n    device = torch.device(\"cuda\")\\nprint(\\'Running on:\\', device)\\n\\n# General pytorch process:\\n# 1) Design model (input_dim, output_dim, forward pass)\\n# 2) Construct loss and optimizer\\n# 3) Training loop: forward: prediction + loss, backward: grads, update weights\\n\\n# hyper parameters:\\ninput_size = 784 # images are 28x28\\nhidden_size = 100\\nnum_classes = 10\\nnum_epochs = 3\\nbatch_size = 100\\nlearning_rate = 0.001 # good practice to try values in 0.01, 0.003, 0.001, 0.0003 etc.\\n\\n# download mnist data, with tensor transform applied\\ntrain_dataset = torchvision.datasets.MNIST(root=\\'./data\\', train = True, transform = transforms.ToTensor(), download = True)\\ntest_dataset = torchvision.datasets.MNIST(root=\\'./data\\', train = False, transform = transforms.ToTensor())\\n\\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle=False)\\n\\n# 1) define model which applies the feed forward method\\nclass NeuralNet(nn.Module):\\n    def __init__(self, input_size, hidden_size, num_classes):\\n        super(NeuralNet, self).__init__()\\n        self.l1 = nn.Linear(input_size, hidden_size) # layer 1\\n        self.relu = nn.ReLU() # activation 1 = ReLU\\n        self.l2 = nn.Linear(hidden_size, num_classes) # layer 2\\n    \\n    def forward(self, x):\\n        out = self.l1(x)\\n        out = self.relu(out)\\n        out = self.l2(out)\\n        #we don\\'t apply softmax activation fn as CrossEntropyLoss does this\\n        return out\\n    \\nmodel = NeuralNet(input_size, hidden_size, num_classes)\\nmodel.to(device)\\n\\n# 2) loss and optimizer\\ncriterion = nn.CrossEntropyLoss() # applies softmax automatically\\noptimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\\n\\n# 3) training loop\\nloss_hist = []\\nn_total_steps = len(train_loader) # 60_000 images = 600 steps\\nfor epoch in range(num_epochs):\\n    for i, (images, labels) in enumerate(train_loader):\\n        images = images.view(-1, input_size) # flatten\\n        images, labels = images.to(device), labels.to(device)\\n        # forward pass\\n        output = model.forward(images)\\n        loss = criterion(output, labels)\\n        loss_hist.append(loss.item())\\n        \\n        # backward pass\\n        loss.backward()\\n        \\n        # update\\n        optimizer.step()\\n        optimizer.zero_grad()\\n        if ((i+1) % 100 == 0):        \\n            print(f\\'epoch: {epoch+1}, step: {i+1}, loss = {loss:.2f}\\')\\n        \\n#test\\nwith torch.no_grad():\\n    n_correct = 0\\n    n_samples = 0\\n    for images, labels in test_loader:\\n        images = images.view(-1, input_size)\\n        images, label = images.to(device), labels.to(device)\\n        outputs = model(images)\\n        _, predictions = torch.max(outputs,1) # returns val, index\\n        n_samples += labels.shape[0]\\n        n_correct += (predictions == labels).sum().item()\\n    \\n    test_acc = (n_correct/n_samples) * 100\\n\\nprint(f\\'test acc: {test_acc:.2f}\\')\\n\\n#plot loss history\\nplt.plot(loss_hist)\\nplt.xlabel(\\'step (60,000 images, 100 per step, 5 epochs)\\')\\nplt.ylabel(\\'train loss (Cross Entropy)\\')\\nplt.title(f\\'Train Loss History (final test accuracy: {test_acc:.2f})\\')\\nplt.savefig(\\'02_output/nn_mnist_training.png\\')\\n\\n# let\\'s save the model\\ntorch.save(model, \\'02_output/simple_nn_mnist.model\\')\\n# load the model with torch.load(model, \\'simple_nn_mnist.model\\')', metadata={'source': '../../temp/repos/stephen-osullivan/pytorch/02_simple_nn_mnist.py'}),\n",
       " Document(page_content='\\'code\\' cell: \\'[\\'import matplotlib.pyplot as plt\\\\n\\', \\'import numpy as np\\\\n\\', \\'import torch\\\\n\\', \\'import torch.nn as nn\\\\n\\', \\'import torch.nn.functional as F\\\\n\\', \\'import torchvision\\\\n\\', \\'import torchvision.transforms as transforms\\']\\'\\n\\n\\'code\\' cell: \\'[\\'#download data\\\\n\\', \\'trainset = torchvision.datasets.CIFAR10(\\\\n\\', \"    root=\\'./data\\', train=True, download=True, transform = transforms.ToTensor())\\\\n\", \\'testset = torchvision.datasets.CIFAR10(\\\\n\\', \"    root=\\'./data\\', train=False, download=True, transform = transforms.ToTensor())\\\\n\", \\'print(trainset) # print trainset info\\']\\'\\n with output: \\'[\\'Files already downloaded and verified\\\\n\\', \\'Files already downloaded and verified\\\\n\\', \\'Dataset CIFAR10\\\\n\\', \\'    Number of datapoints: 50000\\\\n\\', \\'    Root location: ./data\\\\n\\', \\'    Split: Train\\\\n\\', \\'    StandardTransform\\\\n\\', \\'Transform: ToTensor()\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# set up the data loader\\\\n\\', \\'trainloader = torch.utils.data.DataLoader(trainset, batch_size=8, shuffle=True, num_workers=2)\\\\n\\', \\'testloader = torch.utils.data.DataLoader(trainset, batch_size=8, shuffle=False, num_workers=2)\\\\n\\', \"print(\\'Number of batches per epoch:\\', len(trainloader))\"]\\'\\n with output: \\'[\\'Number of batches per epoch: 6250\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# use gpu if available\\\\n\\', \\'device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\\\\n\\', \"print(\\'Using\\', device)\\\\n\", \\'#check\\\\n\\', \\'temp = torch.zeros(10)\\\\n\\', \\'temp.to(device)\\']\\'\\n with output: \\'[\\'Using cpu\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\"class_labels = (\\'plane\\', \\'cars\\', \\'bird\\', \\'cat\\', \\'deer\\', \\'dog\\', \\'frog\\', \\'horse\\', \\'ship\\', \\'truck\\')\"]\\'\\n\\n\\'code\\' cell: \\'[\"# let\\'s load a batch\\\\n\", \\'images, labels = next(iter(trainloader))\\\\n\\', \\'print(images.shape, labels.shape)\\']\\'\\n with output: \\'[\\'torch.Size([8, 3, 32, 32]) torch.Size([8])\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# lets use the makegrid function to view the batch in matplotlib\\\\n\\', \\'img = torchvision.utils.make_grid(images)\\\\n\\', \"print(\\'torchvision grid shape:\\', img.shape)\\\\n\", \\'# matplotlib requires the images to have the number of channels as the last dimension\\\\n\\', \\'img_transposed = np.transpose(img, (1,2,0))\\\\n\\', \"print(\\'transposed numpy grid shape:\\', img_transposed.shape)\\\\n\", \\'plt.figure(figsize=(10,5))\\\\n\\', \\'plt.imshow(img_transposed)\\\\n\\', \"plt.axis(\\'off\\')\\\\n\", \\'plt.show()\\']\\'\\n with output: \\'[\\'torchvision grid shape: torch.Size([3, 36, 274])\\\\n\\', \\'transposed numpy grid shape: torch.Size([36, 274, 3])\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'\\\\n\\', \\'# now we build our model\\\\n\\', \\'in_channels = 3\\\\n\\', \\'h1_channels, h2_channels = 6, 16 # hidden channels\\\\n\\', \\'k_conv_size = 5 # 5x5\\\\n\\', \\'out_dim = len(class_labels)\\\\n\\', \\'\\\\n\\', \"# let\\'s understand how the dimensionality changes:\\\\n\", \"print(\\'Initial Shape:\\'.ljust(25), images.shape)\\\\n\", \\'out = nn.Conv2d(in_channels, h1_channels, k_conv_size)(images)\\\\n\\', \"print(\\'First Conv: (3, 6, 5):\\'.ljust(25), out.shape)\\\\n\", \\'out = nn.MaxPool2d(kernel_size=2)(out)\\\\n\\', \"print(\\'Pool (2x2):\\'.ljust(25), out.shape)\\\\n\", \\'out = nn.Conv2d(h1_channels, h2_channels, k_conv_size)(out)\\\\n\\', \"print(\\'Second Conv (6, 16, 5):\\'.ljust(25), out.shape)\\\\n\", \\'out = nn.MaxPool2d(kernel_size=2)(out)\\\\n\\', \"print(\\'Pool (2x2):\\'.ljust(25), out.shape)\"]\\'\\n with output: \\'[\\'Initial Shape:            torch.Size([8, 3, 32, 32])\\\\n\\', \\'First Conv: (3, 6, 5):    torch.Size([8, 6, 28, 28])\\\\n\\', \\'Pool (2x2):               torch.Size([8, 6, 14, 14])\\\\n\\', \\'Second Conv (6, 16, 5):   torch.Size([8, 16, 10, 10])\\\\n\\', \\'Pool (2x2):               torch.Size([8, 16, 5, 5])\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'learning_rate = 0.003\\\\n\\', \\'num_epochs = 5\\\\n\\', \\'criterion = nn.CrossEntropyLoss() # performs softmax\\\\n\\', \\'optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\\']\\'\\n\\n\\'code\\' cell: \\'[\\'model.train()\\\\n\\', \\'for epoch in range(num_epochs):\\\\n\\', \\'  loss_history = 0\\\\n\\', \\'  for i, (images, labels) in enumerate(trainloader):\\\\n\\', \\'    images, labels = images.to(device), labels.to(device)\\\\n\\', \\'    outputs = model(images)\\\\n\\', \\'\\\\n\\', \\'    optimizer.zero_grad() # wipe gradients\\\\n\\', \\'    loss = criterion(outputs, labels)\\\\n\\', \\'    loss.backward()\\\\n\\', \\'    optimizer.step()\\\\n\\', \\'\\\\n\\', \\'    loss_history += loss.item()\\\\n\\', \\'\\\\n\\', \\'    if((i+1) % 1250 == 0):\\\\n\\', \"      print(f\\'epoch({epoch}), step({i+1}) train_loss: {loss_history/1250:.2f}\\')\\\\n\", \\'      loss_history = 0\\']\\'\\n with output: \\'[\\'epoch(0), step(1250) train_loss: 1.86\\\\n\\', \\'epoch(0), step(2500) train_loss: 1.64\\\\n\\', \\'epoch(0), step(3750) train_loss: 1.55\\\\n\\', \\'epoch(0), step(5000) train_loss: 1.49\\\\n\\', \\'epoch(0), step(6250) train_loss: 1.47\\\\n\\', \\'epoch(1), step(1250) train_loss: 1.41\\\\n\\', \\'epoch(1), step(2500) train_loss: 1.38\\\\n\\', \\'epoch(1), step(3750) train_loss: 1.35\\\\n\\', \\'epoch(1), step(5000) train_loss: 1.33\\\\n\\', \\'epoch(1), step(6250) train_loss: 1.33\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'#test\\\\n\\', \\'model.eval()\\\\n\\', \\'with torch.no_grad():\\\\n\\', \\'    n_correct = 0\\\\n\\', \\'    n_samples = 0\\\\n\\', \\'    for images, labels in testloader:\\\\n\\', \\'        images, label = images.to(device), labels.to(device)\\\\n\\', \\'        outputs = model(images)\\\\n\\', \\'        _, predictions = torch.max(outputs,1) # returns val, index\\\\n\\', \\'        n_samples += labels.shape[0]\\\\n\\', \\'        n_correct += (predictions == labels).sum().item()\\\\n\\', \\'\\\\n\\', \\'    test_acc = (n_correct/n_samples) * 100\\\\n\\', \\'\\\\n\\', \"print(f\\'test acc: {test_acc:.2f}\\')\"]\\'\\n with output: \\'[\\'test acc: 62.73\\\\n\\']\\'\\n\\n', metadata={'source': '../../temp/repos/stephen-osullivan/pytorch/03_cnn_cifar10.ipynb'}),\n",
       " Document(page_content='\\'code\\' cell: \\'[\\'import matplotlib.pyplot as plt\\\\n\\', \\'import numpy as np\\\\n\\', \\'import torch\\\\n\\', \\'import torch.nn as nn\\\\n\\', \\'import torch.nn.functional as F\\\\n\\', \\'import torchvision\\\\n\\', \\'import torchvision.transforms as transforms\\']\\'\\n\\n\\'code\\' cell: \\'[\\'#download data\\\\n\\', \\'trainset = torchvision.datasets.CIFAR10(\\\\n\\', \"    root=\\'./data\\', train=True, download=True, transform = transforms.ToTensor())\\\\n\", \\'testset = torchvision.datasets.CIFAR10(\\\\n\\', \"    root=\\'./data\\', train=False, download=True, transform = transforms.ToTensor())\\\\n\", \\'print(trainset) # print trainset info\\']\\'\\n with output: \\'[\\'Files already downloaded and verified\\\\n\\', \\'Files already downloaded and verified\\\\n\\', \\'Dataset CIFAR10\\\\n\\', \\'    Number of datapoints: 50000\\\\n\\', \\'    Root location: ./data\\\\n\\', \\'    Split: Train\\\\n\\', \\'    StandardTransform\\\\n\\', \\'Transform: ToTensor()\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# set up the data loader\\\\n\\', \\'trainloader = torch.utils.data.DataLoader(trainset, batch_size=8, shuffle=True, num_workers=2)\\\\n\\', \\'testloader = torch.utils.data.DataLoader(trainset, batch_size=8, shuffle=False, num_workers=2)\\\\n\\', \"print(\\'Number of batches per epoch:\\', len(trainloader))\"]\\'\\n with output: \\'[\\'Number of batches per epoch: 6250\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# use gpu if available\\\\n\\', \\'device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\\\\n\\', \"print(\\'Using\\', device)\\\\n\", \\'#check\\\\n\\', \\'temp = torch.zeros(10)\\\\n\\', \\'temp.to(device)\\']\\'\\n with output: \\'[\\'Using cpu\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\"class_labels = (\\'plane\\', \\'cars\\', \\'bird\\', \\'cat\\', \\'deer\\', \\'dog\\', \\'frog\\', \\'horse\\', \\'ship\\', \\'truck\\')\"]\\'\\n\\n\\'code\\' cell: \\'[\"# let\\'s load a batch\\\\n\", \\'images, labels = next(iter(trainloader))\\\\n\\', \\'print(images.shape, labels.shape)\\']\\'\\n with output: \\'[\\'torch.Size([8, 3, 32, 32]) torch.Size([8])\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# lets use the makegrid function to view the batch in matplotlib\\\\n\\', \\'img = torchvision.utils.make_grid(images)\\\\n\\', \"print(\\'torchvision grid shape:\\', img.shape)\\\\n\", \\'# matplotlib requires the images to have the number of channels as the last dimension\\\\n\\', \\'img_transposed = np.transpose(img, (1,2,0))\\\\n\\', \"print(\\'transposed numpy grid shape:\\', img_transposed.shape)\\\\n\", \\'plt.figure(figsize=(10,5))\\\\n\\', \\'plt.imshow(img_transposed)\\\\n\\', \"plt.axis(\\'off\\')\\\\n\", \\'plt.show()\\']\\'\\n with output: \\'[\\'torchvision grid shape: torch.Size([3, 36, 274])\\\\n\\', \\'transposed numpy grid shape: torch.Size([36, 274, 3])\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'\\\\n\\', \\'# now we build our model\\\\n\\', \\'in_channels = 3\\\\n\\', \\'h1_channels, h2_channels = 6, 16 # hidden channels\\\\n\\', \\'k_conv_size = 5 # 5x5\\\\n\\', \\'out_dim = len(class_labels)\\\\n\\', \\'\\\\n\\', \"# let\\'s understand how the dimensionality changes:\\\\n\", \"print(\\'Initial Shape:\\'.ljust(25), images.shape)\\\\n\", \\'out = nn.Conv2d(in_channels, h1_channels, k_conv_size)(images)\\\\n\\', \"print(\\'First Conv: (3, 6, 5):\\'.ljust(25), out.shape)\\\\n\", \\'out = nn.MaxPool2d(kernel_size=2)(out)\\\\n\\', \"print(\\'Pool (2x2):\\'.ljust(25), out.shape)\\\\n\", \\'out = nn.Conv2d(h1_channels, h2_channels, k_conv_size)(out)\\\\n\\', \"print(\\'Second Conv (6, 16, 5):\\'.ljust(25), out.shape)\\\\n\", \\'out = nn.MaxPool2d(kernel_size=2)(out)\\\\n\\', \"print(\\'Pool (2x2):\\'.ljust(25), out.shape)\"]\\'\\n with output: \\'[\\'Initial Shape:            torch.Size([8, 3, 32, 32])\\\\n\\', \\'First Conv: (3, 6, 5):    torch.Size([8, 6, 28, 28])\\\\n\\', \\'Pool (2x2):               torch.Size([8, 6, 14, 14])\\\\n\\', \\'Second Conv (6, 16, 5):   torch.Size([8, 16, 10, 10])\\\\n\\', \\'Pool (2x2):               torch.Size([8, 16, 5, 5])\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'learning_rate = 0.003\\\\n\\', \\'num_epochs = 5\\\\n\\', \\'criterion = nn.CrossEntropyLoss() # performs softmax\\\\n\\', \\'optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\\']\\'\\n\\n\\'code\\' cell: \\'[\\'model.train()\\\\n\\', \\'for epoch in range(num_epochs):\\\\n\\', \\'  loss_history = 0\\\\n\\', \\'  for i, (images, labels) in enumerate(trainloader):\\\\n\\', \\'    images, labels = images.to(device), labels.to(device)\\\\n\\', \\'    outputs = model(images)\\\\n\\', \\'\\\\n\\', \\'    optimizer.zero_grad() # wipe gradients\\\\n\\', \\'    loss = criterion(outputs, labels)\\\\n\\', \\'    loss.backward()\\\\n\\', \\'    optimizer.step()\\\\n\\', \\'\\\\n\\', \\'    loss_history += loss.item()\\\\n\\', \\'\\\\n\\', \\'    if((i+1) % 1250 == 0):\\\\n\\', \"      print(f\\'epoch({epoch}), step({i+1}) train_loss: {loss_history/1250:.2f}\\')\\\\n\", \\'      loss_history = 0\\']\\'\\n with output: \\'[\\'epoch(0), step(1250) train_loss: 1.86\\\\n\\', \\'epoch(0), step(2500) train_loss: 1.64\\\\n\\', \\'epoch(0), step(3750) train_loss: 1.55\\\\n\\', \\'epoch(0), step(5000) train_loss: 1.49\\\\n\\', \\'epoch(0), step(6250) train_loss: 1.47\\\\n\\', \\'epoch(1), step(1250) train_loss: 1.41\\\\n\\', \\'epoch(1), step(2500) train_loss: 1.38\\\\n\\', \\'epoch(1), step(3750) train_loss: 1.35\\\\n\\', \\'epoch(1), step(5000) train_loss: 1.33\\\\n\\', \\'epoch(1), step(6250) train_loss: 1.33\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'#test\\\\n\\', \\'model.eval()\\\\n\\', \\'with torch.no_grad():\\\\n\\', \\'    n_correct = 0\\\\n\\', \\'    n_samples = 0\\\\n\\', \\'    for images, labels in testloader:\\\\n\\', \\'        images, label = images.to(device), labels.to(device)\\\\n\\', \\'        outputs = model(images)\\\\n\\', \\'        _, predictions = torch.max(outputs,1) # returns val, index\\\\n\\', \\'        n_samples += labels.shape[0]\\\\n\\', \\'        n_correct += (predictions == labels).sum().item()\\\\n\\', \\'\\\\n\\', \\'    test_acc = (n_correct/n_samples) * 100\\\\n\\', \\'\\\\n\\', \"print(f\\'test acc: {test_acc:.2f}\\')\"]\\'\\n with output: \\'[\\'test acc: 62.73\\\\n\\']\\'\\n\\n', metadata={'source': '../../temp/repos/stephen-osullivan/pytorch/04_cnn_cifar10_transforms.ipynb'}),\n",
       " Document(page_content='\\'markdown\\' cell: \\'[\\'# Transfer Learning\\\\n\\', \\'\\\\n\\', \"We\\'ll use a pretrained ResNet model to classify flowers.\"]\\'\\n\\n\\'code\\' cell: \\'[\\'import matplotlib.pyplot as plt\\\\n\\', \\'import numpy as np\\\\n\\', \\'import torch\\\\n\\', \\'import torch.nn as nn\\\\n\\', \\'import torch.nn.functional as F\\\\n\\', \\'import torchvision\\\\n\\', \\'from torchvision import datasets, models, transforms\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# use gpu if available\\\\n\\', \\'device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\\\\n\\', \"print(\\'Using\\', device)\"]\\'\\n with output: \\'[\\'Using cuda\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'# Download and Organise Image Files\\']\\'\\n\\n\\'code\\' cell: \\'[\\'#downloaded from kaggle: https://www.kaggle.com/datasets/alxmamaev/flowers-recognition\\\\n\\', \\'import zipfile, os, glob\\\\n\\', \"zipfile.ZipFile(\\'data/flowers.zip\\').extractall(\\'data\\')\\\\n\", \"DATA_DIR = \\'data/flowers\\'\\\\n\", \\'# get the file names\\\\n\\', \\'class_labels = os.listdir(DATA_DIR) \\\\n\\', \\'print(class_labels)\\\\n\\', \\'image_files = glob.glob(os.path.join(DATA_DIR, \"*/*\")) # two directories deep, returns full paths\\\\n\\', \\'len(image_files)\\\\n\\', \"image_labels = [f.split(\\'/\\')[2] for f in image_files] # label for each image (parent folder)\"]\\'\\n with output: \\'[\"[\\'tulip\\', \\'dandelion\\', \\'daisy\\', \\'sunflower\\', \\'rose\\']\\\\n\"]\\'\\n\\n\\'code\\' cell: \\'[\\'# train test split\\\\n\\', \\'from sklearn.model_selection import train_test_split\\\\n\\', \\'\\\\n\\', \\'train_files, test_files, train_labels, test_labels = train_test_split(\\\\n\\', \\'    image_files, image_labels, test_size=0.1, stratify=image_labels)\\\\n\\', \\'train_files, val_files, train_labels, val_labels = train_test_split(\\\\n\\', \\'    train_files, train_labels, test_size=0.1, stratify=train_labels)\\\\n\\', \\'\\\\n\\', \\'print(len(train_files), len(val_files), len(test_files))\\\\n\\', \\'print(len(train_labels), len(val_labels), len(test_labels))\\']\\'\\n with output: \\'[\\'3496 389 432\\\\n\\', \\'3496 389 432\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# now we copy\\\\n\\', \\'import shutil\\\\n\\', \"train_dir = os.path.join(DATA_DIR, \\'train\\')\\\\n\", \"val_dir = os.path.join(DATA_DIR, \\'val\\')\\\\n\", \"test_dir = os.path.join(DATA_DIR, \\'test\\')\\\\n\", \\'\\\\n\\', \\'def copy_files(source_files, dest_dir, labels):\\\\n\\', \\'    # takes the source files, and places them in a folder *dest_dir*\\\\n\\', \\'    # e.g. from data/flowers/class/file.jpg to data/flowers/*dest_dir*/class/file.jpg\\\\n\\', \\'    os.makedirs(dest_dir, exist_ok=True)\\\\n\\', \\'    for label in class_labels:\\\\n\\', \\'        os.makedirs(os.path.join(dest_dir, label), exist_ok=True)\\\\n\\', \"    dest_paths = [os.path.join(dest_dir, f) for f in [\\'/\\'.join(f.split(\\'/\\')[-2:]) for f in source_files]]\\\\n\", \\'    for src, dst in zip(source_files, dest_paths):\\\\n\\', \\'        shutil.copy(src, dst)\\\\n\\', \\'copy_files(train_files, train_dir, class_labels)\\\\n\\', \"print(\\'train copied\\')\\\\n\", \\'copy_files(val_files, val_dir, class_labels)\\\\n\\', \"print(\\'val copied\\')\\\\n\", \\'copy_files(test_files, test_dir, class_labels)\\\\n\\', \"print(\\'test copied\\')\"]\\'\\n with output: \\'[\\'train copied\\\\n\\', \\'val copied\\\\n\\', \\'test copied\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# clean up by removing the old directories we copied the files from\\\\n\\', \\'for label in class_labels:\\\\n\\', \\'    shutil.rmtree(os.path.join(DATA_DIR, label))\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'# Create Pytorch Image Datasets\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# normalization used by pretrained model for the 3 RGB channels\\\\n\\', \\'# see: https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html#torchvision.models.resnet18\\\\n\\', \\'mean = [0.485, 0.456, 0.406]\\\\n\\', \\'std = [0.229, 0.224, 0.225]\\\\n\\', \\'\\\\n\\', \\'train_transform = transforms.Compose([\\\\n\\', \\'    transforms.Resize(256), #smaller image dim matched to this, scale maintained\\\\n\\', \\'    transforms.RandomResizedCrop(224), # this is the final output pixels: 224/224\\\\n\\', \\'    transforms.ToTensor(),\\\\n\\', \\'    transforms.Normalize(mean, std)])\\\\n\\', \\'\\\\n\\', \\'test_transform = transforms.Compose([\\\\n\\', \\'    transforms.Resize(256), \\\\n\\', \\'    transforms.CenterCrop(224), # non random square crop for val/test, output pixels :224x224\\\\n\\', \\'    transforms.ToTensor(),\\\\n\\', \\'    transforms.Normalize(mean, std)])\\\\n\\', \\'\\\\n\\', \\'image_datasets = {}\\\\n\\', \"image_datasets[\\'train\\'] = datasets.ImageFolder(train_dir, train_transform)\\\\n\", \"image_datasets[\\'val\\'] = datasets.ImageFolder(val_dir, test_transform)\\\\n\", \"image_datasets[\\'test\\'] = datasets.ImageFolder(test_dir, test_transform)\\\\n\", \"class_labels = image_datasets[\\'train\\'].classes\\\\n\", \\'\\\\n\\', \\'#sanity checks:\\\\n\\', \\'for k, v in image_datasets.items():\\\\n\\', \"    print(f\\'{k} files:\\', len(v))\"]\\'\\n with output: \\'[\\'train files: 3496\\\\n\\', \\'val files: 389\\\\n\\', \\'test files: 432\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# now we build the dataloaders\\\\n\\', \\'dataloaders = {}\\\\n\\', \\'for k, dataset in image_datasets.items():\\\\n\\', \"    is_train = True if k == \\'train\\' else False\\\\n\", \\'    dataloaders[k] = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=is_train, num_workers=4)\\\\n\\', \\'\\\\n\\', \\'for k, v in dataloaders.items():\\\\n\\', \"    print(f\\'{k} batches:\\', len(v))\"]\\'\\n with output: \\'[\\'train batches: 437\\\\n\\', \\'val batches: 49\\\\n\\', \\'test batches: 54\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'#view example batch\\\\n\\', \\'image_grid = torchvision.utils.make_grid(images)\\\\n\\', \\'image_grid = np.transpose(image_grid, [1,2,0])\\\\n\\', \\'image_grid = image_grid * np.array(std) + np.array(mean)\\\\n\\', \\'plot_title = np.array(class_labels)[labels.numpy()].tolist()\\\\n\\', \\'plt.figure(figsize=(10,5))\\\\n\\', \\'plt.imshow(image_grid)\\\\n\\', \\'plt.title(plot_title)\\\\n\\', \\'plt.show()\\']\\'\\n with output: \\'[\\'Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'# Download Model and Train\\']\\'\\n\\n\\'code\\' cell: \\'[\"model = models.resnet18(weights=\\'IMAGENET1K_V1\\')\\\\n\", \\'#freeze layers\\\\n\\', \\'for param in model.parameters():\\\\n\\', \\'    param.requires_grad = False\\']\\'\\n\\n\\'code\\' cell: \\'[\\'num_features = model.fc.in_features\\\\n\\', \\'print(num_features)\\']\\'\\n with output: \\'[\\'512\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'num_epochs = 10\\\\n\\', \\'lr = 0.03\\\\n\\', \\'model.fc = nn.Linear(num_features, len(class_labels)) # replace the final layer (5 output classes)\\\\n\\', \\'criterion = nn.CrossEntropyLoss()\\\\n\\', \\'optimizer = torch.optim.SGD(model.fc.parameters(), lr=lr, momentum=0.9)\\\\n\\', \\'# reduce learning rate by 0.1 every 5 epochs\\\\n\\', \\'lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) \\']\\'\\n\\n\\'code\\' cell: \\'[\\'\\\\n\\', \\'def train_step(dataloader):\\\\n\\', \\'  model.train()\\\\n\\', \\'  epoch_loss, num_correct, num_samples = 0, 0, 0\\\\n\\', \\'  for i, (images, labels) in enumerate(dataloader):\\\\n\\', \\'    images, labels = images.to(device), labels.to(device)\\\\n\\', \\'    outputs = model(images)\\\\n\\', \\'    \\\\n\\', \\'    optimizer.zero_grad() # wipe gradients\\\\n\\', \\'    loss = criterion(outputs, labels)\\\\n\\', \\'    loss.backward()\\\\n\\', \\'    optimizer.step()\\\\n\\', \\'    \\\\n\\', \\'    \\\\n\\', \\'    #outputs for metrics\\\\n\\', \\'    _, predictions = torch.max(outputs,1) # returns val, index\\\\n\\', \\'    num_samples += len(labels)\\\\n\\', \\'    num_correct += (predictions == labels).sum().item()\\\\n\\', \\'    epoch_loss += loss.item()\\\\n\\', \\'  return epoch_loss/num_samples, num_correct/num_samples\\\\n\\', \\'\\\\n\\', \\'def test_step(dataloader):\\\\n\\', \\'  model.eval()\\\\n\\', \\'  num_correct, num_samples = 0, 0\\\\n\\', \\'  for i, (images, labels) in enumerate(dataloader):\\\\n\\', \\'    with torch.no_grad():\\\\n\\', \\'      images, labels = images.to(device), labels.to(device)\\\\n\\', \\'      outputs = model(images)\\\\n\\', \\'      _, predictions = torch.max(outputs,1) # returns val, index\\\\n\\', \\'      num_samples += len(labels)\\\\n\\', \\'      num_correct += (predictions == labels).sum().item()\\\\n\\', \\'\\\\n\\', \\'  return num_correct/num_samples\\\\n\\', \\'\\\\n\\', \\'model.to(device)\\\\n\\', \\'for epoch in range(num_epochs):\\\\n\\', \"  loss, train_acc = train_step(dataloaders[\\'train\\'])\\\\n\", \"  val_acc = test_step(dataloaders[\\'val\\'])\\\\n\", \\'  lr_scheduler.step()\\\\n\\', \\'    \\\\n\\', \"  print(f\\'epoch({epoch+1})\\',\\\\n\", \"        f\\'train_loss: {loss:.2f}, train_acc: {train_acc:.2f}, val_acc {val_acc:.2f}\\')\"]\\'\\n with output: \\'[\\'epoch(1) train_loss: 0.72, train_acc: 0.63, val_acc 0.89\\\\n\\', \\'epoch(2) train_loss: 0.67, train_acc: 0.72, val_acc 0.87\\\\n\\', \\'epoch(3) train_loss: 0.74, train_acc: 0.71, val_acc 0.85\\\\n\\', \\'epoch(4) train_loss: 0.77, train_acc: 0.72, val_acc 0.85\\\\n\\', \\'epoch(5) train_loss: 0.71, train_acc: 0.73, val_acc 0.87\\\\n\\', \\'epoch(6) train_loss: 0.40, train_acc: 0.80, val_acc 0.89\\\\n\\', \\'epoch(7) train_loss: 0.36, train_acc: 0.80, val_acc 0.89\\\\n\\', \\'epoch(8) train_loss: 0.35, train_acc: 0.80, val_acc 0.88\\\\n\\', \\'epoch(9) train_loss: 0.32, train_acc: 0.81, val_acc 0.90\\\\n\\', \\'epoch(10) train_loss: 0.33, train_acc: 0.81, val_acc 0.89\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# test data\\\\n\\', \"test_acc = test_step(dataloaders[\\'test\\'])\\\\n\", \"print(f\\'test accuracy: {test_acc:.2f}\\')\"]\\'\\n with output: \\'[\\'test accuracy: 0.86\\\\n\\']\\'\\n\\n', metadata={'source': '../../temp/repos/stephen-osullivan/pytorch/05_transfer_learning_cnn.ipynb'}),\n",
       " Document(page_content='\\'markdown\\' cell: \\'[\\'# RNN From Scratch\\\\n\\', \\'Nationality prediction on the names dataset, using characters as tokens. There are 18 nationalities in the dataset.\\\\n\\', \\'\\\\n\\', \\'Based off of:\\\\n\\', \\'https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\\\\n\\', \\'\\\\n\\', \\'1) We download the data and process it.\\\\n\\', \\'2) We build an RNN from scratch\\\\n\\', \\'3) We train the RNN on test and demonstrate 70% accuracy after 200_000 samples.\\\\n\\', \\'4) We define a predict function and use it to create a confusion matrix on the whole dataset.\\\\n\\', \\'5) We use a confusion matrix heatplot to see how the model handled the multi class classification.\\\\n\\', \\'6) We look at some of the worst predictions (model was most certain about but were actually incorrect)\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import glob\\\\n\\', \\'import matplotlib.pyplot as plt\\\\n\\', \\'import numpy as np\\\\n\\', \\'import os\\\\n\\', \\'import pandas as pd\\\\n\\', \\'import torch\\\\n\\', \\'import torch.nn as nn\\\\n\\', \\'import torch.nn.functional as F\\\\n\\', \\'import zipfile\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import unicodedata\\\\n\\', \\'import string\\\\n\\', \\'all_letters = string.ascii_letters + \" .,;\\\\\\'\"\\\\n\\', \"print(\\'letters to be encoded:\\', all_letters)\\\\n\", \\'n_letters = len(all_letters)\\\\n\\', \\'\\\\n\\', \\'# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\\\\n\\', \\'def unicodeToAscii(s):\\\\n\\', \"    return \\'\\'.join(\\\\n\", \"        c for c in unicodedata.normalize(\\'NFD\\', s)\\\\n\", \"        if unicodedata.category(c) != \\'Mn\\'\\\\n\", \\'        and c in all_letters\\\\n\\', \\'    )\\\\n\\', \\'\\\\n\\', \"print(unicodeToAscii(\\'Ślusàrski\\'))\"]\\'\\n with output: \\'[\"letters to be encoded: abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,;\\'\\\\n\", \\'Slusarski\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# we can weight the classes by their inverse class size for training\\\\n\\', \\'class_weights = 1/(class_counts[class_labels].values)\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# lets find the characters appearing in the dataset to confirm they look reasonable\\\\n\\', \\'chars_in_data = set()\\\\n\\', \"for n in data[\\'name\\'].values:\\\\n\", \\'    chars_in_data = chars_in_data.union(list(n))\\\\n\\', \\'chars_in_data = list(chars_in_data)\\\\n\\', \\'chars_in_data = sorted(chars_in_data)\\\\n\\', \"print(\\'characters appearing in the dataset:\\', \\'\\'.join(chars_in_data))\"]\\'\\n with output: \\'[\"characters appearing in the dataset:  \\',ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\\\\n\"]\\'\\n\\n\\'code\\' cell: \\'[\\'# lets build the RNN from scratch:\\\\n\\', \\'class RNN(nn.Module):\\\\n\\', \\'    def __init__(self, input_size, hidden_size, output_size):\\\\n\\', \\'        super(RNN, self).__init__()\\\\n\\', \\'\\\\n\\', \\'        self.hidden_size = hidden_size\\\\n\\', \\'        # input to hidden\\\\n\\', \\'        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\\\\n\\', \\'        # hidden to output\\\\n\\', \\'        self.h2o = nn.Linear(hidden_size, output_size)\\\\n\\', \\'        self.softmax = nn.LogSoftmax(dim=1)\\\\n\\', \\'\\\\n\\', \\'    def forward(self, input, hidden):\\\\n\\', \\'        combined = torch.cat((input, hidden), 1) #concatenate\\\\n\\', \\'        hidden = self.i2h(combined) # concat feeds to hidden\\\\n\\', \\'        output = self.h2o(hidden) # hidden feeds to output\\\\n\\', \\'        output = self.softmax(output) # log probabilities (log softmax)\\\\n\\', \\'        return output, hidden\\\\n\\', \\'\\\\n\\', \\'    def initHidden(self):\\\\n\\', \\'        return torch.zeros(1, self.hidden_size)\\\\n\\', \\'\\\\n\\', \\'n_hidden = 128\\\\n\\', \\'n_categories = len(class_labels)\\\\n\\', \\'rnn = RNN(n_letters, n_hidden, n_categories)\\\\n\\', \\'\\\\n\\', \\'# test on an input\\\\n\\', \\'hidden = rnn.initHidden()\\\\n\\', \"ohe_letter = letter_to_ohe_tensor(\\'a\\')\\\\n\", \\'output, hidden = rnn(ohe_letter, hidden)\\\\n\\', \\'print(output.shape, hidden.shape)\\']\\'\\n with output: \\'[\\'torch.Size([1, 18]) torch.Size([1, 128])\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# define a function to map a natioanlity to a tensor\\\\n\\', \\'nationality_to_idx = {n:idx for idx, n in enumerate(class_labels)}\\\\n\\', \\'def nationality_to_tensor(nationality):\\\\n\\', \\'    return torch.tensor(nationality_to_idx[nationality])\\\\n\\', \\'\\\\n\\', \"print(class_labels[0],\\':\\', nationality_to_tensor(class_labels[0]))\\\\n\", \"print(class_labels[1],\\':\\', nationality_to_tensor(class_labels[1]))\"]\\'\\n with output: \\'[\\'Vietnamese : tensor(0)\\\\n\\', \\'Dutch : tensor(1)\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\\\\n\\', \\'criterion = nn.NLLLoss(weight=torch.tensor(class_weights, dtype=torch.float32))\\\\n\\', \\'\\\\n\\', \\'def train_name(name, nationality):\\\\n\\', \\'    X = name_to_ohe_tensor(name)\\\\n\\', \\'    y = nationality_to_tensor(nationality)\\\\n\\', \\'\\\\n\\', \\'    hidden = rnn.initHidden()\\\\n\\', \\'    rnn.zero_grad()\\\\n\\', \\'    \\\\n\\', \\'    for x in X:\\\\n\\', \\'        output, hidden = rnn(x.reshape(1,-1), hidden)\\\\n\\', \\'\\\\n\\', \\'    loss = criterion(output[0], y)\\\\n\\', \\'    loss.backward()\\\\n\\', \\'\\\\n\\', \"    # Add parameters\\' gradients to their values, multiplied by learning rate\\\\n\", \\'    for p in rnn.parameters():\\\\n\\', \\'        p.data.add_(p.grad.data, alpha=-learning_rate)\\\\n\\', \\'\\\\n\\', \\'    return output, loss.item()\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# train model\\\\n\\', \\'rnn = RNN(n_letters, n_hidden, n_categories)\\\\n\\', \\'\\\\n\\', \\'inputs, labels = data.name.values, data.nationality.values\\\\n\\', \\'iterations = 200_000\\\\n\\', \\'verbosity = int(iterations/10)\\\\n\\', \\'\\\\n\\', \\'n_correct = 0\\\\n\\', \\'running_loss = 0\\\\n\\', \\'for i in range(iterations):\\\\n\\', \\'    idx = np.random.choice(len(inputs))\\\\n\\', \\'    input, label = inputs[idx], labels[idx]\\\\n\\', \\'    output, loss = train_name(input, label)\\\\n\\', \\'    _, pred_label = torch.max(output, 1)\\\\n\\', \\'\\\\n\\', \\'    n_correct += (pred_label.item() == nationality_to_idx[label])\\\\n\\', \\'    running_loss += loss\\\\n\\', \\'    if (i+1) % verbosity == 0:\\\\n\\', \\'        accuracy = n_correct/verbosity\\\\n\\', \\'        running_loss = running_loss/verbosity\\\\n\\', \"        print(f\\'iter {i+1}, loss {running_loss:.2f}, accuracy {accuracy:.2f}\\')\\\\n\", \\'        n_correct, running_loss = 0, 0\\\\n\\']\\'\\n with output: \\'[\\'iter 20000, loss 1.33, accuracy 0.61\\\\n\\', \\'iter 40000, loss 1.01, accuracy 0.70\\\\n\\', \\'iter 60000, loss 0.94, accuracy 0.72\\\\n\\', \\'iter 80000, loss 0.89, accuracy 0.73\\\\n\\', \\'iter 100000, loss 0.86, accuracy 0.74\\\\n\\', \\'iter 120000, loss 0.86, accuracy 0.73\\\\n\\', \\'iter 140000, loss 0.83, accuracy 0.74\\\\n\\', \\'iter 160000, loss 0.82, accuracy 0.74\\\\n\\', \\'iter 180000, loss 0.82, accuracy 0.74\\\\n\\', \\'iter 200000, loss 0.83, accuracy 0.74\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# lets create a predict nationality function for a name\\\\n\\', \\'def predict_nationality(name, model):\\\\n\\', \\'    model.eval()\\\\n\\', \\'    X = name_to_ohe_tensor(name)\\\\n\\', \\'    hidden = rnn.initHidden()\\\\n\\', \\'    with torch.no_grad():\\\\n\\', \\'        for x in X:\\\\n\\', \\'            output, hidden = model(x.reshape(1,-1), hidden)\\\\n\\', \\'    log_prob, idx = torch.max(output, 1)\\\\n\\', \\'    return np.exp(log_prob.item()), class_labels[idx.item()]\\\\n\\', \\'\\\\n\\', \"for name in [\\'George\\', \\'Zhang\\', \\'Vladislav\\', \\'Phu\\', \\'Sayed\\', \\'Reinhardt\\']:\\\\n\", \\'    prob, label = predict_nationality(name, rnn)\\\\n\\', \"    print(f\\'{name} is {label} with probability {prob:.2f}\\')\"]\\'\\n with output: \\'[\\'George is English with probability 0.32\\\\n\\', \\'Zhang is Chinese with probability 0.78\\\\n\\', \\'Vladislav is Russian with probability 1.00\\\\n\\', \\'Phu is Chinese with probability 0.50\\\\n\\', \\'Sayed is Arabic with probability 0.46\\\\n\\', \\'Reinhardt is Russian with probability 0.37\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# lets add a new column to the dataframe\\\\n\\', \"data[\\'prob_pred\\'] = [predict_nationality(name, model = rnn) for name in inputs]\"]\\'\\n\\n\\'code\\' cell: \\'[\"data[\\'prob\\'] = [x[0] for x in data[\\'prob_pred\\']]\\\\n\", \"data[\\'pred\\'] = [x[1] for x in data[\\'prob_pred\\']]\"]\\'\\n\\n', metadata={'source': '../../temp/repos/stephen-osullivan/pytorch/06_rnn_from_scratch.ipynb'}),\n",
       " Document(page_content='matplotlib\\nnumpy\\npandas\\nsklearn\\nseaborn\\ntorch\\ntorchvision\\n', metadata={'source': '../../temp/repos/stephen-osullivan/pytorch/requirements.txt'}),\n",
       " Document(page_content='\\'markdown\\' cell: \\'[\\'<a href=\"https://colab.research.google.com/github/stephen-osullivan/pytorch/blob/main/torch_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\\']\\'\\n\\n\\'code\\' cell: \\'[\\'!pip install -q torch torchvision\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import matplotlib.pyplot as plt\\\\n\\', \\'import numpy as np\\\\n\\', \\'import os\\\\n\\', \\'import time\\\\n\\', \\'import torch\\\\n\\', \\'import torch.nn as nn\\\\n\\', \\'import torch.nn.functional as F\\\\n\\', \\'import torchvision\\\\n\\', \\'from torchvision import datasets, models, transforms\\\\n\\', \\'\\\\n\\', \"device = \\'cuda\\' if torch.cuda.is_available() else \\'cpu\\'\\\\n\", \"print(\\'Using:\\', device)\"]\\'\\n with output: \\'[\\'Using: cuda\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'!wget https://download.pytorch.org/tutorial/hymenoptera_data.zip\\']\\'\\n with output: \\'[\\'--2023-10-04 10:15:19--  https://download.pytorch.org/tutorial/hymenoptera_data.zip\\\\n\\', \\'Resolving download.pytorch.org (download.pytorch.org)... 18.239.225.75, 18.239.225.61, 18.239.225.55, ...\\\\n\\', \\'Connecting to download.pytorch.org (download.pytorch.org)|18.239.225.75|:443... connected.\\\\n\\', \\'HTTP request sent, awaiting response... 200 OK\\\\n\\', \\'Length: 47286322 (45M) [application/zip]\\\\n\\', \\'Saving to: ‘hymenoptera_data.zip’\\\\n\\', \\'\\\\n\\', \\'hymenoptera_data.zi 100%[===================>]  45.10M   137MB/s    in 0.3s    \\\\n\\', \\'\\\\n\\', \\'2023-10-04 10:15:20 (137 MB/s) - ‘hymenoptera_data.zip’ saved [47286322/47286322]\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'!unzip hymenoptera_data.zip\\']\\'\\n with output: \\'[\\'Archive:  hymenoptera_data.zip\\\\n\\', \\'   creating: hymenoptera_data/\\\\n\\', \\'   creating: hymenoptera_data/train/\\\\n\\', \\'   creating: hymenoptera_data/train/ants/\\\\n\\', \\'  inflating: hymenoptera_data/train/ants/0013035.jpg  \\\\n\\', \\'  inflating: hymenoptera_data/train/ants/1030023514_aad5c608f9.jpg  \\\\n\\', \\'  inflating: hymenoptera_data/train/ants/1095476100_3906d8afde.jpg  \\\\n\\', \\'  inflating: hymenoptera_data/train/ants/1099452230_d1949d3250.jpg  \\\\n\\', \\'  inflating: hymenoptera_data/train/ants/116570827_e9c126745d.jpg  \\\\n\\', \\'  inflating: hymenoptera_data/train/ants/1225872729_6f0856588f.jpg  \\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Data augmentation and normalization for training\\\\n\\', \\'# Just normalization for validation\\\\n\\', \\'data_transforms = {\\\\n\\', \"    \\'train\\': transforms.Compose([\\\\n\", \\'        transforms.RandomResizedCrop(224),\\\\n\\', \\'        transforms.RandomHorizontalFlip(),\\\\n\\', \\'        transforms.ToTensor(),\\\\n\\', \\'        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\\\\n\\', \\'    ]),\\\\n\\', \"    \\'val\\': transforms.Compose([\\\\n\", \\'        transforms.Resize(256),\\\\n\\', \\'        transforms.CenterCrop(224),\\\\n\\', \\'        transforms.ToTensor(),\\\\n\\', \\'        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\\\\n\\', \\'    ]),\\\\n\\', \\'}\\\\n\\', \"data_dir = \\'hymenoptera_data\\'\\\\n\", \\'image_datasets = {\\\\n\\', \\'    x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\\\\n\\', \"    for x in [\\'train\\', \\'val\\']}\\\\n\", \\'dataloaders = {\\\\n\\', \\'    x: torch.utils.data.DataLoader(\\\\n\\', \\'        image_datasets[x], batch_size=4,shuffle=True, num_workers=4)\\\\n\\', \"    for x in [\\'train\\', \\'val\\']}\\\\n\", \"dataset_sizes = {x: len(image_datasets[x]) for x in [\\'train\\', \\'val\\']}\\\\n\", \"class_names = image_datasets[\\'train\\'].classes\"]\\'\\n with output: \\'[\\'/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\\\\n\\', \\'  warnings.warn(_create_warning_msg(\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\"os.makedirs(\\'temp\\', exist_ok=True)\\\\n\", \"def train_model(model, criterion, optimizer, scheduler, num_epochs=25, device = \\'cpu\\'):\\\\n\", \\'    model = model.to(device)\\\\n\\', \\'    since = time.time()\\\\n\\', \"    best_model_params_path = os.path.join(\\'temp\\', \\'best_model_params.pt\\')\\\\n\", \\'    torch.save(model.state_dict(), best_model_params_path)\\\\n\\', \\'    best_acc = 0.0\\\\n\\', \\'    for epoch in range(num_epochs):\\\\n\\', \"        print(f\\'Epoch {epoch}/{num_epochs - 1}\\')\\\\n\", \"        print(\\'-\\' * 10)\\\\n\", \\'        # Each epoch has a training and validation phase\\\\n\\', \"        for phase in [\\'train\\', \\'val\\']:\\\\n\", \"            if phase == \\'train\\':\\\\n\", \\'                model.train()  # Set model to training mode\\\\n\\', \\'            else:\\\\n\\', \\'                model.eval()   # Set model to evaluate mode\\\\n\\', \\'            running_loss = 0.0\\\\n\\', \\'            running_corrects = 0\\\\n\\', \\'\\\\n\\', \\'            # Iterate over data.\\\\n\\', \\'            for inputs, labels in dataloaders[phase]:\\\\n\\', \\'                inputs = inputs.to(device)\\\\n\\', \\'                labels = labels.to(device)\\\\n\\', \\'                # zero the parameter gradients\\\\n\\', \\'                optimizer.zero_grad()\\\\n\\', \\'                # forward\\\\n\\', \\'                # track history if only in train\\\\n\\', \"                with torch.set_grad_enabled(phase == \\'train\\'):\\\\n\", \\'                    outputs = model(inputs)\\\\n\\', \\'                    _, preds = torch.max(outputs, 1)\\\\n\\', \\'                    loss = criterion(outputs, labels)\\\\n\\', \\'                    # backward + optimize only if in training phase\\\\n\\', \"                    if phase == \\'train\\':\\\\n\", \\'                        loss.backward()\\\\n\\', \\'                        optimizer.step()\\\\n\\', \\'\\\\n\\', \\'                # statistics\\\\n\\', \\'                running_loss += loss.item() * inputs.size(0)\\\\n\\', \\'                running_corrects += torch.sum(preds == labels.data)\\\\n\\', \"            if phase == \\'train\\':\\\\n\", \\'                scheduler.step()\\\\n\\', \\'\\\\n\\', \\'            epoch_loss = running_loss / dataset_sizes[phase]\\\\n\\', \\'            epoch_acc = running_corrects.double() / dataset_sizes[phase]\\\\n\\', \"            print(f\\'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\\')\\\\n\", \\'\\\\n\\', \\'            # deep copy the model\\\\n\\', \"            if phase == \\'val\\' and epoch_acc > best_acc:\\\\n\", \\'                best_acc = epoch_acc\\\\n\\', \\'                torch.save(model.state_dict(), best_model_params_path)\\\\n\\', \\'\\\\n\\', \\'        print()\\\\n\\', \\'\\\\n\\', \\'        time_elapsed = time.time() - since\\\\n\\', \"        print(f\\'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\\')\\\\n\", \"        print(f\\'Best val Acc: {best_acc:4f}\\')\\\\n\", \\'\\\\n\\', \\'        # load best model weights\\\\n\\', \\'        model.load_state_dict(torch.load(best_model_params_path))\\\\n\\', \\'    return model\\']\\'\\n\\n\\'code\\' cell: \\'[\"model_ft = models.resnet18()#(weights=\\'IMAGENET1K_V1\\')\\\\n\", \\'num_ftrs = model_ft.fc.in_features\\\\n\\', \\'# Here the size of each output sample is set to 2.\\\\n\\', \\'# Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``.\\\\n\\', \\'model_ft.fc = nn.Linear(num_ftrs, 2)\\\\n\\', \\'\\\\n\\', \\'model_ft = model_ft.to(device)\\\\n\\', \\'\\\\n\\', \\'criterion = nn.CrossEntropyLoss()\\\\n\\', \\'\\\\n\\', \\'# Observe that all parameters are being optimized\\\\n\\', \\'optimizer_ft = torch.optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\\\\n\\', \\'\\\\n\\', \\'# Decay LR by a factor of 0.1 every 7 epochs\\\\n\\', \\'exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\\']\\'\\n\\n\\'code\\' cell: \\'[\\'t0 = time.time()\\\\n\\', \\'model_conv = train_model(model_ft, criterion, optimizer_ft,\\\\n\\', \\'                         exp_lr_scheduler, num_epochs=10, device = device)\\']\\'\\n with output: \\'[\\'Epoch 0/9\\\\n\\', \\'----------\\\\n\\']\\'\\n\\n', metadata={'source': '../../temp/repos/stephen-osullivan/pytorch/torch_transfer_learning.ipynb'})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import load_repo_files\n",
    "\n",
    "repo_path = '../../temp/repos/stephen-osullivan/pytorch'\n",
    "docs = load_repo_files(repo_path)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "fp = os.path.join(repo_path, '05_transfer_learning_cnn.ipynb')\n",
    "file_content = list(filter(lambda x : x.metadata['source'] == fp, docs))[0].page_content\n",
    "with open('temp.txt', 'w') as f:\n",
    "    f.flush()\n",
    "    f.writelines(prompt)\n",
    "    f.writelines(f'\\n\\nFILE NAME:\\n\\n')\n",
    "    f.writelines(fp)\n",
    "    f.writelines('\\n\\nFILE CONTENT:\\n\\n')\n",
    "    f.writelines(file_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
